# Llama4 Chat - Deployment Guide

## ğŸ“‹ DescripciÃ³n
Chat interactivo con **Meta Llama 4 Maverick 17B** usando NVIDIA NIM Service. AplicaciÃ³n FastAPI con frontend moderno y responsivo.

## ğŸ—ï¸ Arquitectura
```
llama4-chat/
â”œâ”€â”€ app.py              # AplicaciÃ³n FastAPI principal
â”œâ”€â”€ requirements.txt    # Dependencias Python
â”œâ”€â”€ .env               # Variables de entorno (NO incluir en git)
â”œâ”€â”€ deploy.sh          # Script de deployment para Linux
â”œâ”€â”€ deploy.ps1         # Script de deployment para Windows
â”œâ”€â”€ src/               # CÃ³digo fuente
â”‚   â”œâ”€â”€ nim_client.py  # Cliente NVIDIA NIM API
â”‚   â””â”€â”€ models.py      # Modelos Pydantic
â”œâ”€â”€ config/            # ConfiguraciÃ³n
â”‚   â””â”€â”€ settings.py    # Settings centralizados
â”œâ”€â”€ templates/         # Templates HTML
â”‚   â””â”€â”€ index.html     # Frontend principal
â””â”€â”€ static/           # Archivos estÃ¡ticos
    â”œâ”€â”€ css/styles.css # Estilos CSS
    â””â”€â”€ js/            # JavaScript
        â”œâ”€â”€ chat.js    # LÃ³gica del chat
        â””â”€â”€ ui.js      # GestiÃ³n de UI
```

## ğŸš€ Deployment RÃ¡pido

### OpciÃ³n 1: Script AutomÃ¡tico (Recomendado)

#### En Windows:
```powershell
.\deploy.ps1 -ServerIP "YOUR_DROPLET_IP"
```

#### En el Servidor Linux:
```bash
chmod +x deploy.sh
sudo ./deploy.sh
```

### OpciÃ³n 2: Deployment Manual

#### 1. Subir archivos al servidor
```bash
scp -r llama4-chat/ root@YOUR_DROPLET_IP:/var/www/
```

#### 2. Instalar dependencias
```bash
ssh root@YOUR_DROPLET_IP
cd /var/www/llama4-chat
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

#### 3. Configurar variables de entorno
```bash
# Editar .env con tu API key real
nano .env
```

#### 4. Configurar servicios del sistema
```bash
# El script deploy.sh hace esto automÃ¡ticamente
sudo ./deploy.sh
```

## âš™ï¸ ConfiguraciÃ³n

### Variables de Entorno (.env)
```bash
NVIDIA_API_KEY=tu_api_key_aqui
HOST=127.0.0.1
PORT=8000
DEBUG=False
RELOAD=False
NVIDIA_MODEL=meta/llama-4-maverick-17b-128e-instruct
MAX_TOKENS=1024
DEFAULT_TEMPERATURE=0.7
MAX_CHAR_LIMIT=8000
```

### ConfiguraciÃ³n Nginx
El script de deployment configura automÃ¡ticamente Nginx para:
- Servir la aplicaciÃ³n en el puerto 80
- Proxy reverso a FastAPI (puerto 8000)
- Servir archivos estÃ¡ticos optimizados
- Configurar headers de seguridad

### ConfiguraciÃ³n Supervisor
Gestiona automÃ¡ticamente:
- Auto-inicio del servicio
- Reinicio automÃ¡tico en caso de fallo
- Logs estructurados
- GestiÃ³n de procesos

## ğŸ› ï¸ Operaciones

### Verificar Estado
```bash
sudo supervisorctl status llama4-chat
sudo systemctl status nginx
```

### Ver Logs
```bash
sudo tail -f /var/log/llama4-chat.out.log
sudo tail -f /var/log/llama4-chat.err.log
```

### Reiniciar Servicios
```bash
sudo supervisorctl restart llama4-chat
sudo systemctl restart nginx
```

### Actualizar AplicaciÃ³n
```bash
cd /var/www/llama4-chat
git pull  # Si usas git
source venv/bin/activate
pip install -r requirements.txt
sudo supervisorctl restart llama4-chat
```

## ğŸŒ URLs de Acceso

### ProducciÃ³n
- **Principal**: http://llama4.edbetosolutions.com
- **IP Directa**: http://YOUR_DROPLET_IP

### Desarrollo Local
- **Local**: http://localhost:8000

## ğŸ”’ Seguridad

### Consideraciones
- âœ… API Key configurada como variable de entorno
- âœ… CORS configurado para dominios especÃ­ficos en producciÃ³n
- âœ… Nginx como proxy reverso
- âœ… AplicaciÃ³n ejecutÃ¡ndose como usuario www-data
- âœ… Logs estructurados para auditorÃ­a

### Recomendaciones
- [ ] Configurar SSL/TLS con Let's Encrypt
- [ ] Implementar rate limiting
- [ ] Configurar firewall (UFW)
- [ ] Backup automÃ¡tico de configuraciones

## ğŸ“Š Monitoreo

### MÃ©tricas Disponibles
- Logs de aplicaciÃ³n: `/var/log/llama4-chat.out.log`
- Logs de errores: `/var/log/llama4-chat.err.log`
- Logs de Nginx: `/var/log/nginx/access.log`
- Estado de servicios: `supervisorctl status`

### Endpoints de Salud
- `GET /api/health` - Health check bÃ¡sico
- `GET /api/status` - Estado detallado del servicio
- `GET /api/info` - InformaciÃ³n de la aplicaciÃ³n

## ğŸ› Troubleshooting

### Problemas Comunes

#### Error: Cliente NIM no disponible
```bash
# Verificar API key
grep NVIDIA_API_KEY /var/www/llama4-chat/.env
# Verificar conectividad
curl -H "Authorization: Bearer $NVIDIA_API_KEY" https://integrate.api.nvidia.com/v1/chat/completions
```

#### Error 502 Bad Gateway
```bash
# Verificar que FastAPI estÃ¡ ejecutÃ¡ndose
sudo supervisorctl status llama4-chat
# Verificar logs
sudo tail -f /var/log/llama4-chat.err.log
```

#### Alta latencia
```bash
# Verificar recursos del servidor
htop
df -h
# Verificar logs de Nginx
sudo tail -f /var/log/nginx/access.log
```

## ğŸ“ Soporte
- **Desarrollador**: EdBeto Solutions
- **Docs NVIDIA NIM**: https://docs.nvidia.com/nim/
- **FastAPI Docs**: https://fastapi.tiangolo.com/

---
**VersiÃ³n**: 1.0.0  
**Ãšltima actualizaciÃ³n**: Agosto 2025
